{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import keras.utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation, Permute, GRU, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling2D, Flatten, Conv2D, BatchNormalization, Lambda, Bidirectional, concatenate\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import RMSprop\n",
    "import graphviz\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(77)\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_path = '../input/spect-scaled/all_spects_scaled.npz'\n",
    "with open(spec_path, 'rb') as handle:\n",
    "    all_spects = np.load(handle)['arr_0']\n",
    "    \n",
    "info_path = '../input/track-info/track_info.pickle'\n",
    "with open(info_path, 'rb') as handle:\n",
    "    track_info = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = track_info.index\n",
    "track_id_dict = dict(zip(track_ids,all_spects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_splits(split):\n",
    "    mask = track_info.split == split\n",
    "    ids = track_info.loc[mask].index\n",
    "    data = np.asarray([track_id_dict[i] for i in ids])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_splits(split):\n",
    "    mask = track_info.split == split\n",
    "    labels = track_info.loc[mask, 'labels']\n",
    "    labels = np.asarray([np.array(l) for l in labels])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_dataset_splits('training')\n",
    "X_val = make_dataset_splits('validation')\n",
    "X_test = make_dataset_splits('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "track_info['labels'] = enc.fit_transform(track_info['genre_top'])\n",
    "\n",
    "y_cat = keras.utils.to_categorical(track_info['labels'].values)\n",
    "track_info['labels'] = [y.tolist() for y in y_cat]\n",
    "\n",
    "y_train = make_label_splits('training')\n",
    "y_val = make_label_splits('validation')\n",
    "y_test = make_label_splits('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, model_name,  epochs = 20, batch_size = 64):\n",
    "    \n",
    "    best_model_path = './models/'+ model_name + '/weights.best.h5'\n",
    "    best_model = ModelCheckpoint(best_model_path, monitor='val_acc',\n",
    "                                          save_best_only=True, mode='max')\n",
    "    rp_callback = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10, min_delta=0.0)\n",
    "    \n",
    "    tensorboard_path = './models/'+ model_name + '/logs'\n",
    "    tensorboard = TensorBoard(log_dir = tensorboard_path)\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, epochs = epochs,batch_size = batch_size, validation_data = (X_val,y_val),\n",
    "                    callbacks = [best_model,rp_callback, tensorboard])\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(param_grid):\n",
    "    \n",
    "    n_filters = param_grid['n_filters']\n",
    "    filter_size = param_grid['filter_size']\n",
    "    pool_size = param_grid['pool_size']\n",
    "    dropout_rate = param_grid['dropout_rate']\n",
    "    n_cov_layers = param_grid['n_cov_layers']\n",
    "    n_dense_nodes = param_grid['n_dense_nodes']\n",
    "    input_shape = param_grid['input_shape']\n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(n_cov_layers):\n",
    "        model.add(Conv2D(filters = n_filters, kernel_size = filter_size, activation = 'relu', \n",
    "                         input_shape = (input_shape[-3],input_shape[-2],1), padding = 'same',\n",
    "                        data_format = 'channels_last', name = 'conv2d_' + str(i)))\n",
    "        model.add(BatchNormalization( name = 'batch_norm_' + str(i)))\n",
    "        model.add(Activation('relu', name = 'relu_' + str(i)))\n",
    "        model.add(MaxPooling2D(pool_size = pool_size, data_format = \"channels_last\", name = 'max_pool2d_' + str(i)))\n",
    "        model.add(Dropout(dropout_rate, name = 'dropout_' + str(i)))\n",
    "    \n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "    model.add(Dense(n_dense_nodes , activation = 'relu', name = 'dense_1'))\n",
    "    model.add(Dense(n_dense_nodes , activation = 'relu', name='dense_2'))\n",
    "    model.add(Dense(8, activation = 'softmax', name = 'output'))\n",
    "    \n",
    "    model.compile( loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_filters': 60,\n",
    "             'filter_size': (5,5),\n",
    "             'pool_size': (2,2),\n",
    "              'input_shape': X_train.shape,\n",
    "             'dropout_rate': 0.3,\n",
    "             'n_cov_layers': 5,\n",
    "             'n_dense_nodes':240}\n",
    "\n",
    "cnn1 = cnn(param_grid)\n",
    "cnn1_fitted , cnn_history = train_model(cnn1, X_train, y_train, X_val, y_val, model_name  = 'cnn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_cnn, train_acc_cnn = cnn1.evaluate(X_train, y_train )\n",
    "print('Training set Accuracy: ',train_acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_cnn, val_acc_cnn = cnn1.evaluate(X_val, y_val)\n",
    "print('Validation set Accuracy: ',val_acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_cnn, test_acc_cnn = cnn1.evaluate(X_test, y_test)\n",
    "print('Test set Accuracy: ',test_acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crnn(param_grid):\n",
    "    \n",
    "    n_filters = param_grid['n_filters']\n",
    "    filter_size = param_grid['filter_size']\n",
    "    pool_size = param_grid['pool_size']\n",
    "    dropout_rate = param_grid['dropout_rate']\n",
    "    n_cov_layers = param_grid['n_cov_layers']\n",
    "    n_recurrent_layers = param_grid['n_recurrent_layers']\n",
    "    n_time_layers = param_grid['n_time_layers']\n",
    "    n_dense_nodes = param_grid['n_dense_nodes']\n",
    "    n_time_dst_nodes = param_grid['n_time_dst_nodes']\n",
    "    l2 = param_grid['l2']\n",
    "    output_shape = param_grid['output_shape']\n",
    "    input_shape = param_grid['input_shape']\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(input_shape[-3], input_shape[-2], input_shape[-1]),\n",
    "                       name = 'input')\n",
    "    layer = input_layer\n",
    "    for i in range(n_cov_layers):\n",
    "        layer = Conv2D(filters = n_filters, kernel_size=filter_size, padding='same', \n",
    "                       name = 'conv2d_' + str(i))(layer)\n",
    "        layer = BatchNormalization(axis=3, name = 'batch_norm' + str(i))(layer)\n",
    "        layer = Activation('relu', name = 'relu' + str(i))(layer)\n",
    "        layer = MaxPooling2D(pool_size=pool_size, name = 'pool2d' + str(i))(layer)\n",
    "    \n",
    "    layer = Permute((2, 1, 3), name = 'permute')(layer)\n",
    "    layer = Reshape((input_shape[-2], -1), name = 'reshape')(layer)\n",
    "    layer = LSTM(96, return_sequences=False, name = 'lstm')(layer)\n",
    "    \n",
    "    layer = Dropout(dropout_rate, name = 'dropout_lstm')(layer)\n",
    "    layer = Dense(n_dense_nodes, kernel_regularizer=regularizers.l2(l2), name = 'dense_regularizer')(layer)\n",
    "    \n",
    "\n",
    "    output_layer = Dense(8, activation = 'softmax', name = 'output')(layer)\n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = output_layer)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_param_grid = {'n_filters': 56,\n",
    "                   'filter_size': (2,2),\n",
    "                   'pool_size': (2,2),\n",
    "                   'dropout_rate': 0.2,\n",
    "                   'n_cov_layers': 5, \n",
    "                   'n_recurrent_layers':2,\n",
    "                   'n_time_layers': 1,\n",
    "                   'n_time_dst_nodes': 128,\n",
    "                   'n_dense_nodes':240,\n",
    "                   'l2': .001,\n",
    "                   'output_shape': y_train.shape }\n",
    "\n",
    "crnn1 = crnn(X_train, crnn_param_grid)\n",
    "crnn1_fitted , crnn_history = train_model(crnn1, X_train, y_train, X_val, y_val, model_name = 'crnn', epochs = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_crnn, train_acc_crnn = crnn1.evaluate(X_train, y_train)\n",
    "print('Training set Accuracy: ',train_acc_crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_crnn, val_acc_crnn = crnn1.evaluate(X_val, y_val)\n",
    "print('Validation set Accuracy: ',val_acc_crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_crnn, test_acc_crnn = crnn1.evaluate(X_test, y_test)\n",
    "print('Test set Accuracy: ',test_acc_crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crnn_parallel(input_data, param_grid):\n",
    "    \n",
    "    n_filters = param_grid['n_filters']\n",
    "    filter_size = param_grid['filter_size']\n",
    "    pool_size = param_grid['pool_size']\n",
    "    dropout_rate = param_grid['dropout_rate']\n",
    "    n_cov_layers = param_grid['n_cov_layers']\n",
    "    n_recurrent_layers = param_grid['n_recurrent_layers']\n",
    "    n_time_layers = param_grid['n_time_layers']\n",
    "    n_dense_nodes = param_grid['n_dense_nodes']\n",
    "    n_time_dst_nodes = param_grid['n_time_dst_nodes']\n",
    "    l2 = param_grid['l2']\n",
    "    output_shape = param_grid['output_shape']\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(input_data.shape[-3], input_data.shape[-2], input_data.shape[-1]),\n",
    "                       name = 'input')\n",
    "    layer = input_layer\n",
    "    for i in range(n_cov_layers):\n",
    "        layer = Conv2D(filters = n_filters[i], kernel_size = filter_size, padding = 'valid', \n",
    "                       strides = 1, name = 'conv2d_' + str(i))(layer)\n",
    "        layer = BatchNormalization(axis = 3, name = 'batch_norm' + str(i))(layer)\n",
    "        layer = Activation('relu', name = 'relu' + str(i))(layer)\n",
    "        layer = MaxPooling2D(pool_size = pool_size[i], name = 'pool2d' + str(i))(layer)\n",
    "        layer = Dropout(dropout_rate, name = 'dropout' + str(i))(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    r_layer = MaxPooling2D(pool_size[5], name = 'pool_lstm')(input_layer)\n",
    "    r_layer = Lambda(lambda x: K.squeeze(x, axis= -1))(r_layer)\n",
    "    \n",
    "    r_layer = Bidirectional(GRU(64), merge_mode='concat')(r_layer)\n",
    "\n",
    "    concat = concatenate([layer, r_layer], axis=-1, name ='concat')\n",
    "    \n",
    "    output_layer = Dense(8, activation = 'softmax', name = 'output')(concat)\n",
    " \n",
    "    opt = RMSprop(lr=0.0005)\n",
    "    model = Model(inputs = input_layer, outputs = output_layer)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_parallel_param_grid = {'n_filters': [16,32,64,64,64],\n",
    "                   'filter_size': (3,1),\n",
    "                   'pool_size': [(2,2),(2,2),(2,2),(4,4),(4,4),(4,2)],\n",
    "                   'dropout_rate': 0.3,\n",
    "                   'n_cov_layers': 5, \n",
    "                   'n_recurrent_layers': 2,\n",
    "                   'n_time_layers': 1,\n",
    "                   'n_time_dst_nodes': 128,\n",
    "                   'n_dense_nodes':240,\n",
    "                   'l2': .001,\n",
    "                   'output_shape': y_train.shape }\n",
    "\n",
    "crnn_parallel = crnn_parallel(X_train, crnn_parallel_param_grid)\n",
    "crnn_parallel_fitted , crnn_parallel_history = train_model(crnn_parallel, X_train, y_train, X_val, y_val,\n",
    "                                                           model_name = 'crnn_parallel2',epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_crnn_p, train_acc_crnn_p = crnn_parallel_fitted.evaluate(X_train, y_train)\n",
    "print('Training set Accuracy: ',train_acc_crnn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_crnn_p, val_acc_crnn_p = crnn_parallel_fitted.evaluate(X_val, y_val)\n",
    "print('Validation set Accuracy: ',val_acc_crnn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_crnn_p, test_acc_crnn_p = crnn_parallel_fitted.evaluate(X_test, y_test)\n",
    "print('Test set Accuracy: ',test_acc_crnn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [train_acc_cnn, train_acc_crnn, train_acc_crnn_p]\n",
    "val_acc = [val_acc_cnn, val_acc_crnn, val_acc_crnn_p]\n",
    "test_acc = [train_acc_cnn, test_acc_crnn,test_acc_crnn_p]\n",
    "cols = ['CNN', 'CRNN', 'CRNN Parallel']\n",
    "index = ['Training', 'Validation', 'Testing']\n",
    "data = dict(zip(cols, [train_acc, val_acc, test_acc]))\n",
    "results = pd.DataFrame(data, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = crnn_parallel_fitted.predict(X_train)\n",
    "val_predictions = crnn_parallel_fitted.predict(X_val)\n",
    "test_predictions = crnn_parallel_fitted.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_df = pd.DataFrame(train_predictions, columns = [l + '_prob' for l in enc.classes_], index = track_info.loc[track_info.split == 'training'].index )\n",
    "val_predictions_df = pd.DataFrame(val_predictions, columns = [l + '_prob' for l in enc.classes_], index = track_info.loc[track_info.split == 'validation'].index )\n",
    "test_predictions_df = pd.DataFrame(test_predictions, columns = [l + '_prob' for l in enc.classes_], index = track_info.loc[track_info.split == 'test'].index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_df.to_pickle('train_predictions.pickle')\n",
    "val_predictions_df.to_pickle('val_predictions.pickle')\n",
    "test_predictions_df.to_pickle('test_predictions.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
